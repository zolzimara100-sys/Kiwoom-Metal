#!/usr/bin/env python3
"""
GPU Maximum Load Test
GPU ì‚¬ìš©ë¥ ì„ ìµœëŒ€ë¡œ ì˜¬ë¦¬ëŠ” ì—°ì† ê³„ì‚° í…ŒìŠ¤íŠ¸
"""

import torch
import numpy as np
import time

def gpu_max_load_test(duration_seconds=60):
    """GPU ìµœëŒ€ ë¶€í•˜ í…ŒìŠ¤íŠ¸ - ì§€ì •ëœ ì‹œê°„ë™ì•ˆ ì‹¤í–‰"""

    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print(f"âœ… Metal GPU (MPS) detected")
    else:
        print("âŒ GPU not available")
        return

    print(f"\nğŸ® GPU ìµœëŒ€ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œì‘!")
    print(f"   - Duration: {duration_seconds} seconds")
    print(f"   - Device: {device}")
    print(f"\nâš¡ GPU ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•˜ì„¸ìš”!")
    print(f"   macOS: sudo powermetrics --samplers gpu_power -i 500")
    print(f"   Activity Monitor > Window > GPU History\n")

    time.sleep(3)

    start_time = time.time()
    iteration = 0

    # ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¡œ ì—°ì† GPU ì—°ì‚°
    while (time.time() - start_time) < duration_seconds:
        # ëŒ€ëŸ‰ì˜ ëœë¤ ë°ì´í„° ìƒì„± (ì‹œë®¬ë ˆì´ì…˜: 1000ê°œ ì¢…ëª© x 1000ì¼)
        data_size = 1000000
        data = np.random.randn(data_size).astype(np.float32)

        # GPUë¡œ ì „ì†¡ (optimized with explicit dtype)
        tensor = torch.from_numpy(data).to(device=device, dtype=torch.float32)

        # ì—¬ëŸ¬ ìœˆë„ìš° í¬ê¸°ë¡œ ë™ì‹œ ê³„ì‚° (GPU ë¶€í•˜ ê·¹ëŒ€í™”)
        windows = [5, 10, 20, 30, 40, 50, 60, 90, 120, 200, 300]

        for window in windows:
            # GPU Convolution (Heavy computation, optimized with explicit dtype)
            kernel = torch.ones(window, device=device, dtype=torch.float32) / window
            padded = torch.nn.functional.pad(
                tensor.unsqueeze(0).unsqueeze(0),
                (window-1, 0)
            )
            ma = torch.nn.functional.conv1d(
                padded,
                kernel.unsqueeze(0).unsqueeze(0)
            ).squeeze()

            # ì¶”ê°€ GPU ì—°ì‚° (í–‰ë ¬ ê³±ì…ˆ)
            matrix = torch.randn(1000, 1000).to(device)
            result = torch.matmul(matrix, matrix)

        iteration += 1
        elapsed = time.time() - start_time

        if iteration % 5 == 0:
            print(f"ğŸ”¥ Iteration: {iteration:,} | "
                  f"Elapsed: {elapsed:.1f}s | "
                  f"GPU Load: MAXIMUM | "
                  f"Calculations: {iteration * len(windows):,}")

    total_time = time.time() - start_time

    print(f"\nâœ… GPU Maximum Load Test Complete!")
    print(f"   - Total iterations: {iteration:,}")
    print(f"   - Total calculations: {iteration * len(windows):,}")
    print(f"   - Time: {total_time:.1f} seconds")
    print(f"   - Device: {device}")
    print(f"   - Average GPU operations/sec: {(iteration * len(windows))/total_time:.1f}")

if __name__ == "__main__":
    # 60ì´ˆê°„ GPU ìµœëŒ€ ë¶€í•˜ ì‹¤í–‰
    gpu_max_load_test(duration_seconds=60)
